{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eogcK_IR26bk"
      },
      "source": [
        "# SECTION 0: Install packages if required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAVKIeyq7GpM"
      },
      "source": [
        "Install necessary packages as required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-FZCq3R3EQ5"
      },
      "outputs": [],
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iOjhthqrYsl"
      },
      "outputs": [],
      "source": [
        "pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDlYLe2srht3"
      },
      "outputs": [],
      "source": [
        "pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtKGoL2xrPmh"
      },
      "outputs": [],
      "source": [
        "pip install polyfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huChOkFR7WzN"
      },
      "outputs": [],
      "source": [
        "pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjw3ernE_qRi"
      },
      "outputs": [],
      "source": [
        "pip install polyfuzz[flair]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H7rVoSi7rlG"
      },
      "source": [
        "\n",
        "# SECTION 1: Cleaning the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX9JS-tjg2JR"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pIqpLO1jcHxO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HL4g-16BMI5",
        "outputId": "0e2a1309-6619-4928-da4b-22c2ba20ee94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFmhUJVZ6bFl"
      },
      "source": [
        "Import files from Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt2_-JQ8BUMK"
      },
      "source": [
        "Link for the Dataset:\n",
        "https://drive.google.com/drive/folders/10CQ_BADLLNgALyTwC_18gmYzWL58cpG-?usp=sharing\n",
        "\n",
        "The Google Drive folder is shared, just add it to the base directory of your Drive (\"/content/drive/MyDrive\") and the mounting will work fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSZrDU-onyLM",
        "outputId": "c361d216-8abb-4904-f2bd-f338bb616838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb7bg4uBg6wx"
      },
      "source": [
        "Function to read data from json file to a list. instance_num is used to read the first n data in the json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZnF4VVjxgWSe"
      },
      "outputs": [],
      "source": [
        "def load_dataset_json(path, instance_num = 1e7):\n",
        "    data = []\n",
        "    with open(path, 'r') as openfile:\n",
        "        #data = json.load(openfile)\n",
        "        for iline, line in enumerate(openfile.readlines()):\n",
        "            data.append(json.loads(line))\n",
        "            if iline + 1 >= instance_num:\n",
        "                break\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdCtpP4Qo5DH"
      },
      "source": [
        "Function to read data from Wikipedia articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RULlApiccLe1"
      },
      "outputs": [],
      "source": [
        "def load_wiki_json(path, instance_num = 1e5):\n",
        "    data = []\n",
        "    with open(path, 'r') as openfile:\n",
        "        #data = json.load(openfile)\n",
        "        for iline, line in enumerate(openfile.readlines()):\n",
        "            data.append(json.loads(line))\n",
        "            if iline + 1 >= instance_num:\n",
        "                break\n",
        "    for d in data:\n",
        "      d.pop('lines')            \n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0JC0NV3hKoV"
      },
      "source": [
        "Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SzukwSxcWlO",
        "outputId": "5b5df4ac-6262-4a80-ac9f-18b8d5955afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 89296, 'claim': 'Henry Spencer is played by a Greek actor.'}\n",
            "{'id': 78554, 'claim': 'John Ritter died in October.'}\n",
            "{'id': 83809, 'claim': '13 Reasons Why is the only television series of 2012 in the drama-mystery genre.'}\n",
            "{'id': 49758, 'claim': 'Playboy is a magazine.'}\n",
            "{'id': 22973, 'claim': 'Alternative metal is the genre in which Alice in Chains usually performs.'}\n",
            "{'id': 181494, 'claim': 'Sam Peckinpah directed The Wild Bunch.'}\n",
            "{'id': 161592, 'claim': \"The St. John's water dog is a breed of domestic dog that was first bred in Newfoundland.\"}\n",
            "{'id': 117342, 'claim': 'Horseshoe crabs are not used in fertilizer.'}\n",
            "{'id': 172204, 'claim': 'Sia (musician) has received an award presented by the cable channel MTV.'}\n",
            "{'id': 95552, 'claim': 'Artificial intelligence raises concern.'}\n"
          ]
        }
      ],
      "source": [
        "test_path = '/content/drive/MyDrive/data/fever-data/test.jsonl'\n",
        "test_data = load_dataset_json(path=test_path, instance_num=20)\n",
        "\n",
        "for sample in test_data[:10]:\n",
        "    print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fhe7SiBhPFc"
      },
      "source": [
        "Loading all wikipedia articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srJP3kl5ck6m",
        "outputId": "fe46ba83-6867-4acd-d683-1f3dac6902b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100 % done"
          ]
        }
      ],
      "source": [
        "wiki_data=[]\n",
        "os.chdir('/content/drive/MyDrive/data/wiki-pages')\n",
        "i=1\n",
        "for file in os.listdir():\n",
        "  print('\\r', math.floor(100*i/109), '% done', end = '')\n",
        "  wiki_data.extend(load_wiki_json(path = file))\n",
        "  i += 1\n",
        "os.chdir('..')  \n",
        "os.chdir('..')  \n",
        "wiki_data.remove(wiki_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JPyPQZjhSWi"
      },
      "source": [
        "Cleaning the claims to get tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT8tChhFdILq"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sample in test_data:\n",
        "    tokens =  re.split(r'[ -,._]', sample['claim'])\n",
        "    tokens = list(filter(lambda token: token not in string.punctuation, tokens))\n",
        "    sample['token'] = [lemmatizer.lemmatize(w.lower()) for w in tokens if not w.lower() in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTGxhsp76ukB"
      },
      "source": [
        "**Sample Output**: \n",
        "\n",
        "> Tokens obtained from the claims\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT0Hk2sUNx-u",
        "outputId": "8d3ed5cf-227a-4ad1-96d9-2b6917456db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['henry', 'spencer', 'played', 'greek', 'actor']\n",
            "['john', 'ritter', 'died', 'october']\n",
            "['13', 'reason', 'television', 'series', '2012', 'drama-mystery', 'genre']\n",
            "['playboy', 'magazine']\n",
            "['alternative', 'metal', 'genre', 'alice', 'chain', 'usually', 'performs']\n",
            "['sam', 'peckinpah', 'directed', 'wild', 'bunch']\n",
            "['st', 'john', 'water', 'dog', 'breed', 'domestic', 'dog', 'first', 'bred', 'newfoundland']\n",
            "['horseshoe', 'crab', 'used', 'fertilizer']\n",
            "['sia', 'musician', 'received', 'award', 'presented', 'cable', 'channel', 'mtv']\n",
            "['artificial', 'intelligence', 'raise', 'concern']\n"
          ]
        }
      ],
      "source": [
        "for sample in test_data[:10]:   \n",
        "    print(sample['token'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Jn-vk1hZ1p"
      },
      "source": [
        "Cleaning the wiki articles to get tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3tSc_kRc6Xt",
        "outputId": "cd2d9bb6-199a-4ec0-df2c-62e64da7bec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 10 % done"
          ]
        }
      ],
      "source": [
        "n = len(wiki_data)\n",
        "i = 1\n",
        "prt = 0\n",
        "for sample in wiki_data:\n",
        "  if math.floor(100*(i/n)) == prt:\n",
        "    print('\\r', prt, '% done', end = '')\n",
        "    prt += 1\n",
        "  tokens = re.split(r'[_,–. ]', sample['id'])\n",
        "  tokens = list(filter(lambda token: token not in string.punctuation, tokens))\n",
        "  sample['token'] = [lemmatizer.lemmatize(w.lower()) for w in tokens if not w.lower() in stop_words]\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkZoiHaH6-GI"
      },
      "source": [
        "**Sample Output**: \n",
        "\n",
        "> Tokens obtained from the wiki articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdnvW0TZc7JL",
        "outputId": "f05803e1-6919-4671-e33a-6251674d021f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sin', 'sukju']\n",
            "['south', 'oroville', 'california']\n",
            "['southwest', 'golf', 'classic']\n",
            "['st', \"philip's\", 'cathedral', 'san', 'felipe']\n",
            "['st', \"bartholomew's\", 'church', 'chipping']\n",
            "['soulmate', '-lrb-disambiguation-rrb-']\n",
            "['spanish', 'flu', 'research']\n",
            "['society', 'cultural', 'anthropology']\n",
            "['skip', 'ewing']\n",
            "['somerset', 'academy', '-lrb-pembroke', 'pine', 'florida-rrb-']\n"
          ]
        }
      ],
      "source": [
        "for sample in wiki_data[:10]:\n",
        "    print(sample['token'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nSX_67I8VCp"
      },
      "source": [
        "# SECTION 2: Document Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUpYW9n3gTnc"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hMYas0I_hguY"
      },
      "outputs": [],
      "source": [
        "from polyfuzz.models import TFIDF\n",
        "from polyfuzz import PolyFuzz\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from polyfuzz.models import Embeddings\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "from operator import length_hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHdkSeVw-Dhu"
      },
      "source": [
        "Setting up the thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j4eA_Chb-DLg"
      },
      "outputs": [],
      "source": [
        "fuzz_threshold = 55\n",
        "similarity_threshold = 0.8\n",
        "num_doc = 10\n",
        "num_claim = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMC0nsJunwXt"
      },
      "source": [
        "Document Selection:\n",
        ">  Two models designed:\n",
        "* Tf - Idf\n",
        "* BERT\n",
        "\n",
        ">   Document selection done in 3 layers:\n",
        "1.   Layer 1: Filter out articles which has similarity < fuzz_threshold, using fuzzy matching\n",
        "2.   Layer 2: Get Tf-Idf similarity score\n",
        "3.   Layer 3: Get a similiraty score using a BERT model\n",
        "\n",
        "Each article which crosses the threshold are assigned a score considering the similarity obtained from all 3 layers. Top 10 (can be varied by the variable num_doc) articles are chosen for the next step.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uxIJsva-2iH"
      },
      "source": [
        "Designing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXEJAekexNC"
      },
      "outputs": [],
      "source": [
        "tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)\n",
        "model1 = PolyFuzz(tfidf)\n",
        "embeddings = TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
        "bert = Embeddings(embeddings, min_similarity=0)\n",
        "model2 = PolyFuzz(bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAxV-gjpG1ZZ"
      },
      "source": [
        "Randomly selecting 5 (variable num_claim) claims for which the evidences are retrieved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMjiEpPuGzOj"
      },
      "outputs": [],
      "source": [
        "res = random.sample(range(0, len(test_data)), num_claim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN3ILNQQ-5Qq"
      },
      "source": [
        "Document Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ssH3cykZXB",
        "outputId": "2b9b26fe-480b-4196-b180-d0854c942303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting on claim:  Henry Spencer is played by a Greek actor.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  Sam Peckinpah directed The Wild Bunch.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  Horseshoe crabs are not used in fertilizer.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            " 100 % done\n",
            "\n",
            "Starting on claim:  Brazilian Girls is a group.\n",
            " 100 % done\n",
            "\n"
          ]
        }
      ],
      "source": [
        "document = []\n",
        "for k in res:\n",
        "    print('Starting on claim: ', test_data[k]['claim'])\n",
        "    tup=[]\n",
        "    n = len(wiki_data)\n",
        "    prt = 0\n",
        "    for i in range(n):\n",
        "      if math.floor(100*(i+1)/n) == prt:\n",
        "        print(\"\\r\",prt,'% done',end=\"\")\n",
        "        prt += 1\n",
        "      count = 0\n",
        "      similarity = fuzz.token_sort_ratio(test_data[k]['token'],wiki_data[i]['token'])\n",
        "      if similarity > fuzz_threshold:\n",
        "        model1.match(test_data[k]['token'],wiki_data[i]['token'])\n",
        "        l = length_hint(sample['token'])\n",
        "        count = round(similarity*l*0.01,1)\n",
        "        for p in model1.get_matches()['Similarity']:\n",
        "          if p > similarity_threshold:\n",
        "            count+=1\n",
        "        if count > round(similarity*l*0.01,1):\n",
        "          model2.match(test_data[k]['token'],wiki_data[i]['token'])\n",
        "          for p in model2.get_matches()['Similarity']:\n",
        "            if p > similarity_threshold:\n",
        "              count+=1\n",
        "        if count > 3:\n",
        "          tup.append((i,count)) \n",
        "    tup.sort(key = lambda x: x[1], reverse=True)\n",
        "    document.append(tup[:num_doc])\n",
        "    del tup\n",
        "    print('\\n')\n",
        "del model1\n",
        "del model2    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIbeePSJvLxn"
      },
      "source": [
        "Output for Section 2:\n",
        "\n",
        "**DOCUMENT RETRIEVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DgwsYzlxWunl",
        "outputId": "cad42fcf-a433-457b-fdb2-b6bd0e77b8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claim:  Henry Spencer is played by a Greek actor.\n",
            "(2625886, 7.2) \t\t Henry_Spencer_Ashbee\n",
            "(2638145, 7.2) \t\t Henry_C._Spencer\n",
            "(2660251, 7.2) \t\t Henry_E._Spencer\n",
            "(3469927, 7.2) \t\t Henry_Spencer_Palmer\n",
            "(3566599, 7.2) \t\t Lord_Henry_Spencer\n",
            "(4560605, 7.1) \t\t Henry_Spencer_Berkeley\n",
            "(3096802, 6.9) \t\t List_of_actors_who_have_played_Sherlock_Holmes\n",
            "(4039115, 6.9) \t\t List_of_Greek_actors\n",
            "(4542972, 6.9) \t\t Henry_Elvins_Spencer\n",
            "(4555676, 6.9) \t\t Henry_Spencer\n",
            "\n",
            "\n",
            "Claim:  Sam Peckinpah directed The Wild Bunch.\n",
            "(3974591, 6.9) \t\t Sam_Peckinpah_bibliography\n",
            "(4004022, 6.9) \t\t Sam_Peckinpah\n",
            "(5278822, 6.9) \t\t Butch_Cassidy's_Wild_Bunch\n",
            "(4171832, 4.9) \t\t Bueng_Sam_Phan_District\n",
            "\n",
            "\n",
            "Claim:  Horseshoe crabs are not used in fertilizer.\n",
            "(3937879, 7.2) \t\t Horseshoe_crab\n",
            "(3844797, 7.0) \t\t Mangrove_horseshoe_crab\n",
            "(720281, 6.9) \t\t Atlantic_horseshoe_crab\n",
            "(1603546, 5.5) \t\t Battle_of_the_House_in_the_Horseshoe\n",
            "(3502453, 5.5) \t\t Horseshoe_Crater\n",
            "(3776655, 5.2) \t\t Darling's_horseshoe_bat\n",
            "(3803243, 5.2) \t\t Dent's_horseshoe_bat\n",
            "(4561068, 5.2) \t\t Hill's_horseshoe_bat\n",
            "(98708, 5.1) \t\t Decken's_horseshoe_bat\n",
            "(961523, 5.1) \t\t Fertilizer_subsidies_in_Sub-Saharan_Africa\n",
            "\n",
            "\n",
            "Claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            "(3375142, 9.5) \t\t Portland_Trail_Blazers_Radio_Network\n",
            "(3931100, 9.4) \t\t History_of_the_Portland_Trail_Blazers\n",
            "(3076135, 9.3) \t\t List_of_Portland_Trail_Blazers_seasons\n",
            "(3393476, 9.3) \t\t Portland_Trail_Blazers\n",
            "(3068489, 9.2) \t\t List_of_Portland_Trail_Blazers_head_coaches\n",
            "(3166655, 9.2) \t\t Portland_Trail_Blazers_all-time_roster\n",
            "(3176943, 9.2) \t\t Portland_Trail_Blazers_draft_history\n",
            "(2397244, 9.1) \t\t List_of_Portland_Trail_Blazers_executives\n",
            "(3219012, 8.9) \t\t 2010–11_Portland_Trail_Blazers_season\n",
            "(3220249, 8.9) \t\t 2017–18_Portland_Trail_Blazers_season\n",
            "\n",
            "\n",
            "Claim:  Brazilian Girls is a group.\n",
            "(574703, 8.1) \t\t Brazilian_Girls\n",
            "(2299492, 7.5) \t\t The_Best_of_the_Girl_Groups\n",
            "(562506, 7.4) \t\t Brazilian_Girls_-LRB-album-RRB-\n",
            "(3143620, 7.4) \t\t Girl_group\n",
            "(5329132, 6.9) \t\t List_of_girl_groups\n",
            "(4711711, 6.8) \t\t Confessions_of_a_Brazilian_Call_Girl\n",
            "(581269, 5.9) \t\t Brazilian_Gold_Rush\n",
            "(1541416, 5.8) \t\t Brazilian_hip_hop\n",
            "(580938, 5.6) \t\t Brazilian_rock\n",
            "(586825, 5.6) \t\t Brazilian_real\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for doc in document:\n",
        "  if i>=num_claim:\n",
        "    break\n",
        "  print(\"Claim: \",test_data[res[i]]['claim'])\n",
        "  for d in doc:\n",
        "    print(d,'\\t\\t',wiki_data[d[0]]['id'])\n",
        "  print('\\n')\n",
        "  i+=1  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCLGlj_m_Gz_"
      },
      "source": [
        "# SECTION 3: Sentence Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtIhne89_GI1"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBhIaTigZLY_"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMMk8pBY_gjY"
      },
      "source": [
        "Setting number of evidence sentences required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxn1MLdr_cvS"
      },
      "outputs": [],
      "source": [
        "num_sentence = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7iEIDxx_UnG"
      },
      "source": [
        "Results obtained from normal string matching without any model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wyrYc7f-YXI9",
        "outputId": "f91eab81-6133-4295-fde5-a48f480d481b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claim:  Henry Spencer is played by a Greek actor.\n",
            "Evidence: \n",
            "This is a list of Greek actors .\n",
            "Henry Spencer -LRB- born 1955 -RRB- is a Canadian computer programmer and space enthusiast .\n",
            "The list of actors who have played Sherlock Holmes in film , television , stage , or radio includes :\n",
            "He is coauthor , with David Lawrence , of the book Managing Usenet .\n",
            "Spencer was succeeded as mayor by Mark P. Taylor in 1851 .\n",
            "\n",
            "\n",
            "Claim:  Sam Peckinpah directed The Wild Bunch.\n",
            "Evidence: \n",
            "A list of books and essays about Sam Peckinpah :   Peckinpah\n",
            "Peckinpah 's combative personality , marked by years of alcohol and drug abuse , affected his professional legacy .\n",
            "He was given the nickname `` Bloody Sam '' owing to the violence in his films .\n",
            "It was popularized by the 1969 movie , Butch Cassidy and the Sundance Kid , and took its name from the original Wild Bunch .\n",
            "Peckinpah 's films generally deal with the conflict between values and ideals , and the corruption of violence in human society .\n",
            "\n",
            "\n",
            "Claim:  Horseshoe crabs are not used in fertilizer.\n",
            "Evidence: \n",
            "There are four extant species of horseshoe crab .\n",
            "They are commonly used as bait and in fertilizer .\n",
            "All four extant species of horseshoe crabs are anatomically very similar .\n",
            "The other three extant species in the family Limulidae are also called horseshoe crabs .\n",
            "Horseshoe crabs are marine arthropods of the family Limulidae and order Xiphosura or Xiphosurida .\n",
            "\n",
            "\n",
            "Claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            "Evidence: \n",
            "This is a list of Portland Trail Blazers executives , since the team 's foundation in 1970 .\n",
            "Canales was named interim coach of the Trail Blazers toward the end of the season .\n",
            "There have been 14 head coaches for the Trail Blazers franchise .\n",
            "The Trail Blazers are owned by Paul Allen , and Neil Olshey is their general manager .\n",
            "The Trail Blazers first participated in the NBA Draft on March 23 , 1970 , before their inaugural NBA season .\n",
            "\n",
            "\n",
            "Claim:  Brazilian Girls is a group.\n",
            "Evidence: \n",
            "Brazilian Girls is the first album by Brazilian Girls .\n",
            "Brazilian hip hop is a national music genre .\n",
            "This is a list of girl groups of all musical genres .\n",
            "Minas Gerais was the gold mining center of Brazil .\n",
            "reais -RRB- is the present-day currency of Brazil .\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for doc in document:\n",
        "  if i>=num_claim:\n",
        "    break\n",
        "  print('Claim: ',test_data[res[i]]['claim'])\n",
        "  print('Evidence: ')\n",
        "  evidence=[]\n",
        "  for d in doc:\n",
        "    token_text = sent_tokenize(wiki_data[d[0]]['text'])\n",
        "    for s in token_text:\n",
        "      t = re.split(r'[_,–. ]', s)\n",
        "      t = list(filter(lambda token: token not in string.punctuation, t))\n",
        "      tok = [lemmatizer.lemmatize(w.lower()) for w in t if not w.lower() in stop_words]\n",
        "      similarity = fuzz.token_sort_ratio(test_data[res[i]]['token'],tok)+d[1]\n",
        "      evidence.append((s,similarity))\n",
        "  evidence.sort(key = lambda x: x[1], reverse=True)\n",
        "  if len(evidence) < 1:\n",
        "    print('Not enough evidence')\n",
        "  for e in evidence[:num_sentence]:\n",
        "    print(e[0])\n",
        "  print('\\n')\n",
        "  i+=1  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncj0KZPj__Vv"
      },
      "source": [
        "Sentence Selection:\n",
        ">  Two models designed:\n",
        "* Tf - Idf\n",
        "* BERT\n",
        "\n",
        ">   Sentence selection done in 2 layers:\n",
        "1.   Layer 1: Get Tf-Idf similarity score\n",
        "2.   Layer 2: Get a similiraty score using a BERT model\n",
        "\n",
        "Each claim - candidate sentence pair passed thorough both the layers. Top 5 (can be varied by the variable num_sentences) evidence sentences are chosen as the final result.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpYeEh8_yiR"
      },
      "source": [
        "Design the 2 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFj3NtVOYtPG"
      },
      "outputs": [],
      "source": [
        "embeddings = TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
        "bert = Embeddings(embeddings, min_similarity=0)\n",
        "model4 = PolyFuzz(bert)\n",
        "tfidf = TFIDF(n_gram_range=(3, 3))\n",
        "model3 = PolyFuzz(tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLkboz4kAfex"
      },
      "source": [
        "## Final Output from the Project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XvJuG5xpkgYB",
        "outputId": "3d8459b0-832d-462a-82c8-eb8777d106c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######################\n",
            "Claim:  Henry Spencer is played by a Greek actor.\n",
            "Evidence: \n",
            "Henry Spencer Ashbee -LRB- 21 April 1834 -- 29 July 1900 -RRB- was a book collector , writer , and bibliographer .\n",
            "Henry Christian Spencer -LRB- 1915 -- 2000 -RRB- was an American chemical engineer and executive at the Kerite Company in Seymour , Connecticut .\n",
            "Henry Evans Spencer -LRB- born June 13 , 1807 in Columbia - now part of Cincinnati -RRB- was a notable Cincinnati resident and was Mayor of Cincinnati from 1843-1851 .\n",
            "Major General Henry Spencer Palmer -LRB- 30 April 1838 -- 10 February 1893 -RRB- was a British army military engineer and surveyor , noted for his work in developing Yokohama harbor in the Empire of Japan as a foreign advisor to the Japanese government\n",
            "Lord Henry John Spencer -LRB- 20 December 1770 -- 3 July 1795 -RRB- was a British diplomat and politician .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  Sam Peckinpah directed The Wild Bunch.\n",
            "Evidence: \n",
            "David Samuel `` Sam '' Peckinpah -LRB- -LSB- ˈpɛkɪnˌpɑː -RSB- February 21 , 1925 -- December 28 , 1984 -RRB- was an American film director and screenwriter who achieved prominence following the release of the Western epic The Wild Bunch -LRB- 1969 -RRB- .\n",
            "A list of books and essays about Sam Peckinpah :   Peckinpah\n",
            "Butch Cassidy 's Wild Bunch was one of the loosely organized outlaw gangs operating out of the Hole-in-the-Wall in Wyoming during the Old West era in the United States .\n",
            "It was popularized by the 1969 movie , Butch Cassidy and the Sundance Kid , and took its name from the original Wild Bunch .\n",
            "Peckinpah 's films generally deal with the conflict between values and ideals , and the corruption of violence in human society .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  Horseshoe crabs are not used in fertilizer.\n",
            "Evidence: \n",
            "Horseshoe crabs are marine arthropods of the family Limulidae and order Xiphosura or Xiphosurida .\n",
            "Horseshoe crabs live primarily in and around shallow ocean waters on soft sandy or muddy bottoms .\n",
            "They are commonly used as bait and in fertilizer .\n",
            "Because of their origin 450 million years ago , horseshoe crabs are considered living fossils .\n",
            "The mangrove horseshoe crab -LRB- Carcinoscorpius rotundicauda -RRB- is a chelicerate arthropod found in marine and brackish waters .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  The Portland Trail Blazers have thrice gone to the NBA Finals.\n",
            "Evidence: \n",
            "Jack Ramsay , who was the Trail Blazers head coach from 1976 to 1986 , had the number 77 retired in honor of Portland 's only NBA Finals victory in 1977 .\n",
            "The Portland Trail Blazers Radio Network is an American radio network consisting of 18 radio stations which carry coverage of the Portland Trail Blazers , a professional basketball team in the NBA .\n",
            "Ramsay is the only coach to win an NBA championship with the Trail Blazers , in the 1977 NBA Finals .\n",
            "The following is a list of players , both past and current , who appeared in at least one game for the Portland Trail Blazers NBA franchise .\n",
            "The 2010 -- 11 Portland Trail Blazers season was the 41st season of the franchise in the National Basketball Association -LRB- NBA -RRB- .\n",
            "\n",
            "\n",
            "######################\n",
            "Claim:  Brazilian Girls is a group.\n",
            "Evidence: \n",
            "Brazilian Girls is a band from New York City known for their eclectic blend of electronic dance music with musical styles as diverse as tango , chanson , house , reggae and lounge -LRB- but no Brazilian rhythms at all -RRB- .\n",
            "The Best of the Girl Groups is a 2-volume compilation series released by Rhino Records in 1990 .\n",
            "The collection , compiling 36 of the better known tracks by girl groups of the 1960s , shares spot # 422 in  Rolling Stones list of `` Greatest Albums of All Time '' .\n",
            "The New York Times recommends both volumes , in conjunction with Rhino 's Girl Group Greats , for listeners seeking `` the biggest girl-group hits '' .\n",
            "Brazilian Girls is the first album by Brazilian Girls .\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "k = 0\n",
        "for doc in document:\n",
        "  if k>=num_claim:\n",
        "    break\n",
        "  print('######################')\n",
        "  print('Claim: ',test_data[res[k]]['claim'])\n",
        "  print('Evidence: ')\n",
        "  evidence=[]\n",
        "  for d in doc:\n",
        "    token_text = sent_tokenize(wiki_data[d[0]]['text'])\n",
        "    for s in token_text:\n",
        "      t = re.split(r'[_,–. ]', s)\n",
        "      t = list(filter(lambda token: token not in string.punctuation, t))\n",
        "      tok = [lemmatizer.lemmatize(w.lower()) for w in t if not w.lower() in stop_words]\n",
        "      if len(tok) < 2:\n",
        "        continue\n",
        "      model3.match(test_data[res[k]]['token'],tok)\n",
        "      count = 0\n",
        "      l = length_hint(test_data[res[k]]['token'])\n",
        "      for i in model3.get_matches()['Similarity']:\n",
        "        if i > similarity_threshold:\n",
        "          count+=i\n",
        "      if count>0:\n",
        "        model4.match(test_data[res[k]]['token'],tok)\n",
        "        for i in model4.get_matches()['Similarity']:\n",
        "          if i > similarity_threshold:\n",
        "            count+=i\n",
        "      evidence.append((s,count))\n",
        "  evidence.sort(key = lambda x: x[1], reverse=True)\n",
        "  if len(evidence) < 1:\n",
        "    print('Not enough evidence\\n')\n",
        "    k+=1\n",
        "    continue\n",
        "  for e in evidence[:num_sentence]:\n",
        "    print(e[0])\n",
        "  print('\\n')\n",
        "  k+=1  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ACCURACY (*OPTIONAL)"
      ],
      "metadata": {
        "id": "29sUHoG2FV1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test accuracy by testing on the train set. Choosing some random samples from the training set, we can run our model on it and comparing them manually with the evidence already provided in them."
      ],
      "metadata": {
        "id": "LPloEcHWQOdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/data/fever-data/train.jsonl'\n",
        "train_data = load_dataset_json(path=train_path, instance_num=20)"
      ],
      "metadata": {
        "id": "XWTwH7DEFTkd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for sample in train_data:\n",
        "    tokens =  re.split(r'[ -,._]', sample['claim'])\n",
        "    tokens = list(filter(lambda token: token not in string.punctuation, tokens))\n",
        "    sample['token'] = [lemmatizer.lemmatize(w.lower()) for w in tokens if not w.lower() in stop_words]"
      ],
      "metadata": {
        "id": "K4T2SgjxFfKX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)\n",
        "model1 = PolyFuzz(tfidf)\n",
        "embeddings = TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
        "bert = Embeddings(embeddings, min_similarity=0)\n",
        "model2 = PolyFuzz(bert)"
      ],
      "metadata": {
        "id": "1RPTU2fbFlsf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = random.sample(range(0, len(train_data)), num_claim)"
      ],
      "metadata": {
        "id": "CL2YcWRBFroN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in res:\n",
        "  print('#####################')\n",
        "  for s1 in train_data[i]['evidence']:\n",
        "    for s2 in s1:\n",
        "      print(s2[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx-6HKkXKOfg",
        "outputId": "bb4067a4-da33-450f-e380-8f89cdf551dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################\n",
            "Roman_Atwood\n",
            "Roman_Atwood\n",
            "#####################\n",
            "History_of_art\n",
            "#####################\n",
            "Stranger_Things\n",
            "#####################\n",
            "Nikolaj_Coster-Waldau\n",
            "Fox_Broadcasting_Company\n",
            "#####################\n",
            "Ryan_Gosling\n",
            "Chad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = []\n",
        "for k in res:\n",
        "    print('Starting on claim: ', train_data[k]['claim'])\n",
        "    tup=[]\n",
        "    n = len(wiki_data)\n",
        "    prt = 0\n",
        "    for i in range(n):\n",
        "      if math.floor(100*(i+1)/n) == prt:\n",
        "        print(\"\\r\",prt,'% done',end=\"\")\n",
        "        prt += 1\n",
        "      count = 0\n",
        "      similarity = fuzz.token_sort_ratio(train_data[k]['token'],wiki_data[i]['token'])\n",
        "      if similarity > fuzz_threshold:\n",
        "        model1.match(train_data[k]['token'],wiki_data[i]['token'])\n",
        "        l = length_hint(sample['token'])\n",
        "        count = round(similarity*l*0.01,1)\n",
        "        for p in model1.get_matches()['Similarity']:\n",
        "          if p > similarity_threshold:\n",
        "            count+=1\n",
        "        if count > round(similarity*l*0.01,1):\n",
        "          model2.match(train_data[k]['token'],wiki_data[i]['token'])\n",
        "          for p in model2.get_matches()['Similarity']:\n",
        "            if p > similarity_threshold:\n",
        "              count+=1\n",
        "        if count > 3:\n",
        "          tup.append((i,count)) \n",
        "    tup.sort(key = lambda x: x[1], reverse=True)\n",
        "    document.append(tup[:num_doc])\n",
        "    del tup\n",
        "    print('\\n')\n",
        "del model1\n",
        "del model2    "
      ],
      "metadata": {
        "id": "JfW7aNvvFvwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for doc in document:\n",
        "  if i>=num_claim+1:\n",
        "    break\n",
        "  print(\"Claim: \",train_data[res[i]]['claim'])\n",
        "  for d in doc:\n",
        "    print(d,'\\t\\t',wiki_data[d[0]]['id'])\n",
        "  print('\\n')\n",
        "  i+=1  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeTbBRhILtr7",
        "outputId": "4bc02804-ccb5-4b2f-db06-e78e7e70198e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim:  Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\n",
            "(4022682, 7.4) \t\t List_of_Manila_Broadcasting_Company_stations\n",
            "(2087124, 7.3) \t\t Far_East_Broadcasting_Company\n",
            "\n",
            "\n",
            "Claim:  Roman Atwood is a content creator.\n",
            "(3242020, 5.4) \t\t Acts_of_Roman_Congregations\n",
            "(4822710, 5.4) \t\t Content_creation\n",
            "(5123468, 4.4) \t\t Romani_contemporary_art\n",
            "\n",
            "\n",
            "Claim:  History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.\n",
            "\n",
            "\n",
            "Claim:  Adrienne Bailon is an accountant.\n",
            "(4586735, 7.6) \t\t Adrienne_Bailon\n",
            "(3263190, 5.6) \t\t ASEAN_Federation_of_Accountants\n",
            "(3264933, 5.6) \t\t Accountant_General\n",
            "(1783151, 5.5) \t\t International_Federation_of_Accountants\n",
            "(2890988, 5.5) \t\t Chartered_Accountants_Ireland\n",
            "(4616484, 5.5) \t\t Accountant_General_of_the_Federation\n",
            "(4681547, 5.5) \t\t Certified_National_Accountant\n",
            "(993021, 5.4) \t\t Forensic_accountant\n",
            "(1882480, 5.4) \t\t Pan_African_Federation_of_Accountants\n",
            "(2058077, 5.4) \t\t National_Board_of_Accountants_and_Auditors\n",
            "\n",
            "\n",
            "Claim:  System of a Down briefly disbanded in limbo.\n",
            "\n",
            "\n",
            "Claim:  Homeland is an American television spy thriller based on the Israeli television series Prisoners of War.\n",
            "(3079138, 13.5) \t\t List_of_British_television_series_based_on_American_television_series\n",
            "(3086218, 13.5) \t\t List_of_American_television_series_based_on_British_television_series\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 0\n",
        "for doc in document:\n",
        "  if k>=num_claim+1:\n",
        "    break\n",
        "  print('######################')\n",
        "  print('Claim: ',train_data[res[k]]['claim'])\n",
        "  print('Evidence: ')\n",
        "  evidence=[]\n",
        "  for d in doc:\n",
        "    token_text = sent_tokenize(wiki_data[d[0]]['text'])\n",
        "    for s in token_text:\n",
        "      t = re.split(r'[_,–. ]', s)\n",
        "      t = list(filter(lambda token: token not in string.punctuation, t))\n",
        "      tok = [lemmatizer.lemmatize(w.lower()) for w in t if not w.lower() in stop_words]\n",
        "      if len(tok) < 2:\n",
        "        continue\n",
        "      model3.match(train_data[res[k]]['token'],tok)\n",
        "      count = 0\n",
        "      l = length_hint(train_data[res[k]]['token'])\n",
        "      for i in model3.get_matches()['Similarity']:\n",
        "        if i > similarity_threshold:\n",
        "          count+=i\n",
        "      if count>0:\n",
        "        model4.match(train_data[res[k]]['token'],tok)\n",
        "        for i in model4.get_matches()['Similarity']:\n",
        "          if i > similarity_threshold:\n",
        "            count+=i\n",
        "      evidence.append((s,count,wiki_data[d[0]]['id']))\n",
        "  evidence.sort(key = lambda x: x[1], reverse=True)\n",
        "  if len(evidence) < 1:\n",
        "    print('Not enough evidence\\n')\n",
        "    k+=1\n",
        "    continue\n",
        "  for e in evidence[:num_sentence]:\n",
        "    print(e[2],'\\n',e[0])\n",
        "  print('\\n')\n",
        "  k+=1  "
      ],
      "metadata": {
        "id": "S9usx6qCF2kT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IR_Project5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
